# PyLabFastAPI/app/utils/llm_factory.py
import os
import logging
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
# ğŸ‘‡ [æ ¸å¿ƒä¿®æ”¹] V3 ç‰ˆæœ¬å¿…é¡»ä» .langchain å¯¼å…¥ CallbackHandler
from langfuse.langchain import CallbackHandler
from app.config import settings

# ğŸ‘‡ğŸ‘‡ğŸ‘‡ [æ–°å¢] å¼€å¯ Langfuse è°ƒè¯•æ¨¡å¼ï¼Œè®©å®ƒæŠŠæŠ¥é”™åå‡ºæ¥
os.environ["LANGFUSE_DEBUG"] = "True"
logging.getLogger("langfuse").setLevel(logging.DEBUG)
class LLMFactory:
    """
    LangChain æ¨¡å‹å·¥å‚ï¼šç»Ÿä¸€ç”Ÿäº§å¸¦ LangFuse ç›‘æ§çš„ LLM å®ä¾‹
    """

    @staticmethod
    def get_langfuse_handler():
        """è·å– LangFuse å›è°ƒå¤„ç†å™¨"""
        # åªæœ‰å½“é…ç½®äº† Key æ—¶æ‰å¯ç”¨ï¼Œé˜²æ­¢æŠ¥é”™
        if os.getenv("LANGFUSE_SECRET_KEY") and os.getenv("LANGFUSE_PUBLIC_KEY"):
            return [CallbackHandler(
                secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
                public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
                host=os.getenv("LANGFUSE_HOST")
            )]
        return []

    @staticmethod
    def get_llm(temperature=0.7):
        """
        è‡ªåŠ¨åˆ¤æ–­ä½¿ç”¨ DeepSeek (åœ¨çº¿) è¿˜æ˜¯ Ollama (æœ¬åœ°)
        """
        api_key = getattr(settings, "DEEPSEEK_API_KEY", "")
        callbacks = LLMFactory.get_langfuse_handler()

        # === æ–¹æ¡ˆ A: DeepSeek åœ¨çº¿ API (èµ° OpenAI åè®®) ===
        if api_key:
            return ChatOpenAI(
                model="deepseek-chat",  # æˆ– deepseek-reasoner (R1)
                openai_api_key=api_key,
                openai_api_base="https://api.deepseek.com",
                temperature=temperature,
                callbacks=callbacks,
                verbose=True
            )

        # === æ–¹æ¡ˆ B: æœ¬åœ° Ollama ===
        print("âš ï¸ æœªæ£€æµ‹åˆ° DeepSeek Keyï¼Œæ­£åœ¨ä½¿ç”¨æœ¬åœ° Ollama...")
        return ChatOllama(
            model="deepseek-r1:7b",
            base_url="http://localhost:11434",
            temperature=temperature,
            callbacks=callbacks,
            verbose=True
        )