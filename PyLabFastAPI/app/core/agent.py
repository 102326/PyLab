# PyLabFastAPI/app/core/agent.py
import logging
from uuid import UUID
from typing import AsyncGenerator
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from app.utils.llm_factory import LLMFactory
from app.models.user import User
from app.tools.user import get_user_tools
from app.services.chat_service import ChatService  # ğŸ‘ˆ å¼•å…¥æ··åˆå­˜å‚¨æœåŠ¡

logger = logging.getLogger(__name__)


class PyLabAgent:
    """
    PyLab ä¸šåŠ¡æ™ºèƒ½ä½“ (æ”¯æŒæŒä¹…åŒ–ä¼šè¯)
    """

    def __init__(self, user: User, session_id: UUID):
        self.user = user
        self.session_id = session_id  # ç»‘å®šä¼šè¯
        self.llm = LLMFactory.get_llm(temperature=0.3)
        self.tools = get_user_tools(user)

        # System Prompt
        self.system_prompt = (
            f"ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç¼–ç¨‹æ•™è‚²åŠ©æ‰‹ PyLab AIã€‚å½“å‰ç”¨æˆ·: {self.user.nickname}ã€‚"
            "é»˜è®¤ä½¿ç”¨ä¸­æ–‡ã€‚è¯·åŸºäºæä¾›çš„å†å²ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚"
            "ä¸¥ç¦ç¼–é€ ç”¨æˆ·ä¿¡æ¯ã€‚"
        )

        # åˆ›å»º Graph
        # checkpointer=MemorySaver() è¿™é‡Œä»…ç”¨äº"å•è½®å¯¹è¯å†…"çš„å¤šæ­¥æ€è€ƒçŠ¶æ€ä¿å­˜
        # è·¨è½®æ¬¡çš„å†å²è®°å½•ç”± astream_chat æ‰‹åŠ¨æ³¨å…¥
        self.graph = create_react_agent(
            model=self.llm,
            tools=self.tools,
            prompt=self.system_prompt,
            checkpointer=MemorySaver()
        )

    async def astream_chat(self, user_input: str) -> AsyncGenerator[str, None]:
        """
        å…¨è‡ªåŠ¨æµç¨‹ï¼šåŠ è½½å†å² -> æ³¨å…¥ä¸Šä¸‹æ–‡ -> æ¨ç† -> æµå¼è¾“å‡º
        """
        # 1. ä» Redis/DB è·å–å†å²è®°å½• (List[Dict])
        # æ¯”å¦‚: [{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': 'hello'}]
        history_dicts = await ChatService.get_history(self.session_id)

        # 2. è½¬æ¢ä¸º LangChain æ¶ˆæ¯å¯¹è±¡
        langchain_msgs = []
        for msg in history_dicts:
            if msg['role'] == 'user':
                langchain_msgs.append(HumanMessage(content=msg['content']))
            elif msg['role'] in ['ai', 'assistant']:
                langchain_msgs.append(AIMessage(content=msg['content']))

        # 3. è¿½åŠ å½“å‰ç”¨æˆ·çš„æ–°é—®é¢˜
        langchain_msgs.append(HumanMessage(content=user_input))

        # 4. æ„é€ è¾“å…¥
        # æ³¨æ„ï¼šLangGraph çš„ prebuilt agent èƒ½å¤Ÿè¯†åˆ« messages åˆ—è¡¨å¹¶è‡ªåŠ¨å¤„ç†ä¸Šä¸‹æ–‡
        inputs = {"messages": langchain_msgs}

        # è¿™é‡Œçš„ thread_id ä»…åŒºåˆ†å†…å­˜ä¸­çš„æ‰§è¡Œçº¿ç¨‹ï¼Œé˜²æ­¢å¹¶å‘ä¸²å°
        config = {"configurable": {"thread_id": str(self.session_id)}}

        try:
            # 5. æ‰§è¡Œæ¨ç†æµ
            async for event in self.graph.astream_events(inputs, config=config, version="v2"):
                # ç­›é€‰ LLM ç”Ÿæˆçš„æ–‡æœ¬å—
                if event["event"] == "on_chat_model_stream":
                    content = event["data"]["chunk"].content
                    if content:
                        yield content

        except Exception as e:
            logger.error(f"Agent Error: {e}")
            yield f"âš ï¸ æ€è€ƒè¿‡ç¨‹ä¸­æ–­: {str(e)}"