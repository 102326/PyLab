# PyLabFastAPI/app/core/agent.py
import logging
from uuid import UUID
from typing import AsyncGenerator
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from app.utils.llm_factory import LLMFactory
from app.models.user import User
from app.tools.user import get_user_tools
from app.services.chat_service import ChatService  # ğŸ‘ˆ å¼•å…¥æ··åˆæœåŠ¡

logger = logging.getLogger(__name__)


class PyLabAgent:
    def __init__(self, user: User, session_id: UUID):
        self.user = user
        self.session_id = session_id
        self.llm = LLMFactory.get_llm(temperature=0.3)
        self.tools = get_user_tools(user)

        self.system_prompt = (
            f"ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç¼–ç¨‹æ•™è‚²åŠ©æ‰‹ PyLab AIã€‚å½“å‰ç”¨æˆ·: {self.user.nickname}ã€‚"
            "é»˜è®¤ä½¿ç”¨ä¸­æ–‡ã€‚è¯·åŸºäºæä¾›çš„å†å²ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚"
        )

        # è¿™é‡Œçš„ checkpointer ä»…ç”¨äºç®¡ç†å•æ¬¡æ¨ç†è¿‡ç¨‹ä¸­çš„çŠ¶æ€
        # é•¿æœŸè®°å¿†ç”± ChatService + Redis æ‰˜ç®¡
        self.graph = create_react_agent(
            model=self.llm,
            tools=self.tools,
            prompt=self.system_prompt,
            checkpointer=MemorySaver()
        )

    async def astream_chat(self, user_input: str) -> AsyncGenerator[str, None]:
        # 1. è·å–æ··åˆå­˜å‚¨ä¸­çš„å†å²è®°å½• (Redis -> DB)
        history_dicts = await ChatService.get_history(self.session_id)

        # 2. è½¬æ¢ä¸º LangChain æ¶ˆæ¯æ ¼å¼
        langchain_msgs = []
        for msg in history_dicts:
            if msg['role'] == 'user':
                langchain_msgs.append(HumanMessage(content=msg['content']))
            elif msg['role'] in ['ai', 'assistant']:
                langchain_msgs.append(AIMessage(content=msg['content']))

        # 3. è¿½åŠ å½“å‰é—®é¢˜
        langchain_msgs.append(HumanMessage(content=user_input))

        # 4. æ‰§è¡Œæ¨ç†
        # æ³¨æ„ï¼šæˆ‘ä»¬å°†å†å²ç›´æ¥æ³¨å…¥ messagesï¼ŒLangGraph ä¼šè‡ªåŠ¨å¤„ç†ä¸Šä¸‹æ–‡
        inputs = {"messages": langchain_msgs}
        config = {"configurable": {"thread_id": str(self.session_id)}}

        try:
            async for event in self.graph.astream_events(inputs, config=config, version="v2"):
                if event["event"] == "on_chat_model_stream":
                    content = event["data"]["chunk"].content
                    if content:
                        yield content
        except Exception as e:
            logger.error(f"Agent Error: {e}")
            yield f"âš ï¸ Error: {str(e)}"