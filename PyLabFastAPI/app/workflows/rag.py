# PyLabFastAPI/app/workflows/rag.py
from typing import TypedDict
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableConfig  # ğŸ‘ˆ 1. å¼•å…¥ç±»å‹ï¼Œè§£å†³çˆ†é»„
# æ ¹æ®ä½ ä¹‹å‰çš„æµ‹è¯•ï¼Œobserve ç›´æ¥ä» langfuse å¯¼å…¥
from langfuse import observe
from app.utils.llm_factory import LLMFactory
from app.services.vector_db import VectorDBService


# === 1. å®šä¹‰çŠ¶æ€ (State) ===
class RAGState(TypedDict):
    question: str  # ç”¨æˆ·é—®é¢˜
    context: str  # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    answer: str  # æœ€ç»ˆç­”æ¡ˆ


# === 2. å®šä¹‰èŠ‚ç‚¹ (Nodes) ===

@observe(name="RAG-Retrieve")  # ğŸ‘ˆ ç›‘æ§æ£€ç´¢æ­¥éª¤
async def retrieve_node(state: RAGState):
    """
    æ£€ç´¢èŠ‚ç‚¹ï¼šå»å‘é‡æ•°æ®åº“æŸ¥èµ„æ–™
    """
    question = state["question"]
    try:
        # è°ƒç”¨ä¹‹å‰çš„ VectorDBService
        results = await VectorDBService.search_similar_courses(question, limit=3)
        if not results:
            context = "ï¼ˆèµ„æ–™åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼‰"
        else:
            context = "\n".join([f"- ã€Š{r['title']}ã€‹: {r['desc']}" for r in results])
    except Exception as e:
        context = f"æ£€ç´¢å‡ºé”™: {str(e)}"

    return {"context": context}


@observe(name="RAG-Generate")  # ğŸ‘ˆ ç›‘æ§ç”Ÿæˆæ­¥éª¤
async def generate_node(state: RAGState):
    """
    ç”ŸæˆèŠ‚ç‚¹ï¼šè°ƒç”¨å¤§æ¨¡å‹å›ç­”
    """
    question = state["question"]
    context = state["context"]

    # 1. è·å– LLM
    llm = LLMFactory.get_llm(temperature=0.6)

    # 2. è·å–å›è°ƒ (ç”¨äºç»Ÿè®¡ Tokenï¼Œè™½ç„¶ observe ä¹Ÿèƒ½è®°ï¼Œä½†è¿™ä¸ªå¯¹ LLM æ¶ˆè€—ç»Ÿè®¡æ›´å‡†)
    callbacks = LLMFactory.get_langfuse_handler()

    # 3. å®šä¹‰ Prompt
    prompt = ChatPromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¼–ç¨‹å¯¼å¸ˆã€‚è¯·åŸºäºä¸‹æ–¹çš„ã€è¯¾ç¨‹èµ„æ–™ã€‘å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚

    ã€è¯¾ç¨‹èµ„æ–™ã€‘ï¼š
    {context}

    ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
    {question}

    è¦æ±‚ï¼š
    1. é€»è¾‘æ¸…æ™°ï¼Œé€‚åˆæ–°æ‰‹ã€‚
    2. å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ï¼Œå¹¶å°è¯•ç”¨ä½ çš„é€šç”¨çŸ¥è¯†è¡¥å……ï¼ˆä½†è¦è¯´æ˜è¿™æ˜¯è¡¥å……çŸ¥è¯†ï¼‰ã€‚
    3. ä»…è¾“å‡ºå›ç­”å†…å®¹ï¼Œä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹æ ‡ç­¾ã€‚
    """)

    # 4. æ„å»ºé“¾
    chain = prompt | llm | StrOutputParser()

    # 5. [è§£å†³çˆ†é»„] æ„é€ æ ‡å‡†çš„ RunnableConfig å¯¹è±¡
    run_config: RunnableConfig = {
        "callbacks": callbacks,
        "run_name": "DeepSeek-R1-Call"
    }

    # 6. æ‰§è¡Œ
    answer = await chain.ainvoke(
        {"question": question, "context": context},
        config=run_config  # ğŸ‘ˆ ç°åœ¨ IDE åº”è¯¥ä¸ä¼šæŠ¥é”™äº†
    )

    return {"answer": answer}


# === 3. æ„å»ºå›¾ (Graph) ===
def build_rag_graph():
    workflow = StateGraph(RAGState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("retrieve", retrieve_node)
    workflow.add_node("generate", generate_node)

    # å®šä¹‰è¾¹ (æµç¨‹æ§åˆ¶)
    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", END)

    return workflow.compile()


# å•ä¾‹æ¨¡å¼
rag_app = build_rag_graph()